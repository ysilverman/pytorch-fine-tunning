{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MaSHiIyGd23"
      },
      "source": [
        "# Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcPTRljhXhhI"
      },
      "source": [
        "In this assignment, you will implement a fruit detector.\n",
        "The task is divided into steps for simpler navigation.\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcggFBc_FdC2",
        "outputId": "54447559-7dda-4969-dae9-0728b000e5e7"
      },
      "source": [
        "# we will need this library to process the labeling\n",
        "! pip install xmltodict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (0.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLZlCBW0yD0V",
        "outputId": "69e47c5a-a368-4845-de9a-98b002f29be1"
      },
      "source": [
        "! pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTOiuplEugf"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import xmltodict\n",
        "import json\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfbyE1_tIL1x"
      },
      "source": [
        "## Step 0. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgTskp7RbqFZ"
      },
      "source": [
        "First, let's load the data that you can download [here](https://drive.google.com/file/d/1Ve5e9qdy_sUCMM4qXWrw8ecURg2af9Cm/view?usp=sharing).\n",
        "\n",
        "We have already written a dataset class for you and we encourage you to figure out how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ir9Cbc92OlC",
        "outputId": "9581cd4e-01eb-4cfd-f019-d1ab55a0e2bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LelLza0-FR_Y"
      },
      "source": [
        "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
        "\n",
        "\n",
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        self.transform = transform\n",
        "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
        "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
        "\n",
        "            # self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB).astype(np.float64))\n",
        "\n",
        "            image = cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB).astype(np.float64)\n",
        "            # 1 channel is RGB, the other ones are w and h. Needed for current model\n",
        "            image = image.reshape((image.shape[2], image.shape[0], image.shape[1]))\n",
        "            self.images.append(image)\n",
        "\n",
        "            with open(annotation) as f:\n",
        "                annotation_dict = xmltodict.parse(f.read())\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
        "            if not isinstance(objects, list):\n",
        "                objects = [objects]\n",
        "            for obj in objects:\n",
        "                bndbox = obj[\"bndbox\"]\n",
        "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
        "                bbox = list(map(int, bbox))\n",
        "                bboxes.append(torch.tensor(bbox))\n",
        "                labels.append(class2tag[obj[\"name\"]])\n",
        "            labels = torch.ones(len(objects), dtype=torch.int64)\n",
        "            self.annotations.append(\n",
        "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.transform:\n",
        "            # the following code is correct if you use albumentations\n",
        "            # if you use torchvision transforms you have to modify it\n",
        "            res = self.transform(\n",
        "                image=self.images[i],\n",
        "                bboxes=self.annotations[i][\"boxes\"],\n",
        "                labels=self.annotations[i][\"labels\"],\n",
        "            )\n",
        "            return res[\"image\"], {\n",
        "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
        "                \"labels\": torch.tensor(res[\"labels\"]),\n",
        "            }\n",
        "        else:\n",
        "            return self.images[i], self.annotations[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k21XFH5p_iO"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        # mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        # mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        # obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        # obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        # masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OCL52YoiigR"
      },
      "source": [
        "## Step 1. Intersection over Union (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsybDfSlIrFL"
      },
      "source": [
        "In the [Object Detection task](https://en.wikipedia.org/wiki/Object_detection), you need to find objects of a certain class on the image and locate their positions (using the bounding box). The  model should predict the coordinates of the bounding box `[x0, y0, x1, y1]` and the label for this box. The model can predict multiple candidate bounding boxes for an object. We will select candidates using [Intersection Over Union](https://en.wikipedia.org/wiki/Jaccard_index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otUTl7TMi6Ts"
      },
      "source": [
        "<img src=https://upload.wikimedia.org/wikipedia/commons/c/c7/Intersection_over_Union_-_visual_equation.png>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPZqTTOgjX_K"
      },
      "source": [
        "Implement a function that will calculate IoU for bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2fIzQbRqD5H"
      },
      "source": [
        "# type(debug_gt_bbox[0])\n",
        "# type(debug_dt_bbox[0])\n",
        "# gt_bbox = debug_gt_bbox[0]\n",
        "# dt_bbox = debug_dt_bbox[0]\n",
        "# gt_bbox[2] <= dt_bbox[0] or dt_bbox[2] <= gt_bbox[0]\n",
        "# debug_gt_bbox[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtCPABU8dXOz"
      },
      "source": [
        "debug_gt_bbox = []\n",
        "debug_dt_bbox = []\n",
        "\n",
        "def log_debug(bbox):\n",
        "    print(type(bbox))\n",
        "    if isinstance(bbox, list):\n",
        "        print(len(bbox))\n",
        "    else:\n",
        "        print(bbox.shape)\n",
        "    for b in bbox:\n",
        "        print(b)\n",
        "    print()\n",
        "\n",
        "def intersection_over_union(dt_bbox, gt_bbox):\n",
        "    \"\"\"\n",
        "    Intersection over Union between two bboxes\n",
        "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1] x0 - xmin, x1 - xmax, ...\n",
        "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
        "    :return : intersection over union\n",
        "    \"\"\"\n",
        "    # debug_gt_bbox.append(gt_bbox)\n",
        "    # debug_dt_bbox.append(dt_bbox)\n",
        "\n",
        "    if gt_bbox[2] <= dt_bbox[0] or dt_bbox[2] <= gt_bbox[0]:\n",
        "      return 0\n",
        "    if gt_bbox[3] <= dt_bbox[1] or dt_bbox[3] <= gt_bbox[1]:\n",
        "      return 0\n",
        "    x_d = min(dt_bbox[2], gt_bbox[2]) - max(dt_bbox[0], gt_bbox[0])\n",
        "    y_d = min(dt_bbox[3], gt_bbox[3]) - max(dt_bbox[1], gt_bbox[1])\n",
        "    intersection = x_d * y_d\n",
        "    union = (\n",
        "        (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1]) +\n",
        "        (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1]) -\n",
        "        intersection\n",
        "    )\n",
        "    iou = intersection / union\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2XtWBs9jq1I"
      },
      "source": [
        "If the function is implemented correctly, then the execution of the following cell will produce:\n",
        "\n",
        "**0.14285714285714285**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXZFFOTBjjBX",
        "outputId": "9f023c47-6dcf-4aea-e795-261540ff33c4"
      },
      "source": [
        "dt_bbox = [0, 0, 2, 2]\n",
        "gt_bbox = [1, 1, 3, 3]\n",
        "intersection_over_union(dt_bbox, gt_bbox)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14285714285714285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loadw2Krkq_a"
      },
      "source": [
        "## Step 2. Evaluate Sample (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrVulAQLidT"
      },
      "source": [
        "We now have to evaluate the predictions of the model. To do this, we will write a function that will do the following:\n",
        "1. Take model predictions and ground truth bounding boxes and labels as inputs.\n",
        "2. For each bounding box from the prediction, find the closest bounding box among the answers.\n",
        "3. For each found pair of bounding boxes, check whether the IoU is greater than a certain threshold `iou_threshold`. If the **IoU** exceeds the threshold, then we consider this answer as **True Positive**.\n",
        "4. Remove a matched bounding box from the evaluation.\n",
        "5. For each predicted bounding box, return the detection score and whether we were able to match it or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK54_evGzyQg"
      },
      "source": [
        "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
        "    # ground truth\n",
        "    gt_bboxes = target_true['boxes'].numpy()\n",
        "    gt_labels = target_true['labels'].numpy()\n",
        "\n",
        "    # predictions\n",
        "    dt_bboxes = target_pred['boxes'].numpy()\n",
        "    dt_labels = target_pred['labels'].numpy()\n",
        "    dt_scores = target_pred['scores'].numpy()\n",
        "\n",
        "    results = []\n",
        "    # for each bounding box from the prediction, find the closest bounding box among the answers\n",
        "    # print('evaluate sample ', len(dt_labels))\n",
        "    for detection_id in range(len(dt_labels)):\n",
        "        dt_bbox = dt_bboxes[detection_id, :]\n",
        "        dt_label = dt_labels[detection_id]\n",
        "        dt_score = dt_scores[detection_id]\n",
        "\n",
        "        detection_result_dict = {'score': dt_score}\n",
        "\n",
        "        max_IoU = 0\n",
        "        max_gt_id = -1\n",
        "        for valid_id in range(len(gt_labels)):\n",
        "            cur_iou = intersection_over_union(dt_bbox, gt_bboxes[valid_id])\n",
        "            if cur_iou > max_IoU:\n",
        "              max_IoU = cur_iou\n",
        "              max_gt_id = valid_id\n",
        "\n",
        "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
        "            # mark as True Positive\n",
        "            detection_result_dict['TP'] = 1\n",
        "            # delete matched bounding box\n",
        "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
        "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
        "            # print(True)\n",
        "        else:\n",
        "            detection_result_dict['TP'] = 0\n",
        "            # print(False)\n",
        "\n",
        "        results.append(detection_result_dict)\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgZX7BqUPUtk"
      },
      "source": [
        "## Step 3. Evaluate Model (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5eWfC29RCKJ"
      },
      "source": [
        "To assess the quality of the model, we will use the [mAP](https://jonathan-hui.medium.com/\\map-mean-average-precision-for-object-detection-45c121a31173) metric defined as AP Area under the curve. To do this, you will need to calculate `recall` and` precision`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK04btkgQAq3"
      },
      "source": [
        "from sklearn.metrics import auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOZ2wtpo9wY3"
      },
      "source": [
        "# results = debug_results[-1]\n",
        "# nbr_boxes = debug_nbr[-1]\n",
        "\n",
        "# precision = []\n",
        "# recall = []\n",
        "\n",
        "# tp_so_far = 0\n",
        "# for e, r in enumerate(results):\n",
        "#     if r['TP']:\n",
        "#       tp_so_far += 1\n",
        "#     precision.append(tp_so_far / (e + 1))\n",
        "#     recall.append(tp_so_far / nbr_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ymi9R9f-H4F"
      },
      "source": [
        "# results = debug_results[-2]\n",
        "# results\n",
        "# sorted(results, key=lambda k: k['score'], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmyc8gImoLyu"
      },
      "source": [
        "# val_dataloader\n",
        "# for images, targets_true in val_dataloader:\n",
        "#   print(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhmZOkQajpwZ"
      },
      "source": [
        "debug_targets_pred = []\n",
        "debug_targets_true = []\n",
        "debug_results = []\n",
        "debug_nbr = []\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    # print('evaluate')\n",
        "\n",
        "    def log_debug(target):\n",
        "        print(type(target))\n",
        "        if isinstance(target, list):\n",
        "            print(len(target))\n",
        "        else:\n",
        "            print(target.shape)\n",
        "        for t in target:\n",
        "            print(type(t))\n",
        "            if isinstance(t, np.ndarray):\n",
        "                print(t.shape)\n",
        "        print()\n",
        "\n",
        "    results = []\n",
        "    model.eval()\n",
        "    nbr_boxes = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets_true in test_loader:\n",
        "            images = images[0]\n",
        "            images = list(image.unsqueeze(0).to(device).float() for image in images)\n",
        "            targets_pred = model(images)\n",
        "\n",
        "            targets_true = [{k: v.cpu().float() for k, v in t.items()} for t in targets_true]\n",
        "            # result of batch_size == 1?\n",
        "            # targets_true = [{k: v.cpu().float()[0] for k, v in t.items()} for t in [targets_true]]\n",
        "            targets_pred = [{k: v.cpu().float() for k, v in t.items()} for t in targets_pred]\n",
        "\n",
        "            for i in range(len(targets_true)):\n",
        "                target_true = targets_true[i]\n",
        "                target_pred = targets_pred[i]\n",
        "                nbr_boxes += target_true['labels'].shape[0]\n",
        "\n",
        "                results.extend(evaluate_sample(target_pred, target_true))\n",
        "\n",
        "            # print(len(results))\n",
        "\n",
        "    results = sorted(results, key=lambda k: k['score'], reverse=True)\n",
        "    if not len(results):\n",
        "        return None\n",
        "\n",
        "    # compute precision and recall to calculate mAP\n",
        "\n",
        "    precision = []\n",
        "    recall = []\n",
        "\n",
        "    debug_results.append(results)\n",
        "    debug_nbr.append(nbr_boxes)\n",
        "\n",
        "    tp_so_far = 0\n",
        "    counter = 0\n",
        "    for r in results:\n",
        "        if r['TP']:\n",
        "            tp_so_far += 1\n",
        "        precision.append(tp_so_far / (counter + 1))\n",
        "        recall.append(tp_so_far / nbr_boxes)\n",
        "        counter += 1\n",
        "\n",
        "    return auc(recall, precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d464lPHgltbB",
        "outputId": "a8f66aca-db5c-4d74-dc84-95b1b0c309bc"
      },
      "source": [
        "debug_results\n",
        "\n",
        "# targets_true = debug_targets_true[-1]\n",
        "# targets_pred = debug_targets_pred[-1]\n",
        "# targets_true = [{k: v.cpu().float()[0] for k, v in t.items()} for t in [targets_true]]\n",
        "# targets_pred = [{k: v.cpu().float() for k, v in t.items()} for t in targets_pred]\n",
        "# print(targets_true[0]['labels'])\n",
        "# print(targets_pred[0]['labels'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4quQZewevyvp"
      },
      "source": [
        "## Step 4. Train functions (30 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNBQcU_cSgts"
      },
      "source": [
        "Now define the functions for training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcE2VFD43KjM"
      },
      "source": [
        "# images, targets = next(iter(train_dataloader))\n",
        "# images = list(image.to(device).float() for image in images)\n",
        "# debug_targets.append(targets)\n",
        "# targets = [{k: v.to(device)[0] for k, v in t.items()} for t in [targets]]\n",
        "# loss_dict = model(images, targets)\n",
        "# losses = sum(loss for loss in loss_dict.values())\n",
        "# loss = losses_reduced.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwZ4lr9xwuxS"
      },
      "source": [
        "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
        "\n",
        "    def f(x):\n",
        "        if x >= warmup_iters:\n",
        "            return 1\n",
        "        alpha = float(x) / warmup_iters\n",
        "        return warmup_factor * (1 - alpha) + alpha\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26JW24UJSmaq"
      },
      "source": [
        "debug_targets = []\n",
        "\n",
        "def train_one_epoch(model, train_dataloader, optimizer, device, epoch):\n",
        "    # YOUR CODE HERE\n",
        "    # TRAIN YOUR MODEL ON THE train_dataloader\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(train_dataloader) - 1)\n",
        "\n",
        "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in train_dataloader:\n",
        "        images = list(image.to(device).float() for image in images)\n",
        "        targets = [{k: v.to(device)[0] for k, v in t.items()} for t in targets]\n",
        "        # result of batch_size == 1?\n",
        "        # targets = [{k: v.to(device)[0] for k, v in t.items()} for t in [targets]]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # optimizer.zero_grad()\n",
        "        # losses.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # if lr_scheduler is not None:\n",
        "        #     lr_scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # if logits.grad is not None:\n",
        "        #     logits.grad.zero_()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device, n_epochs=10):\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"EPOCH: %s\" % epoch)\n",
        "        model.to(device)\n",
        "\n",
        "        # model.train()\n",
        "        train_one_epoch(model, train_dataloader, optimizer, device=device, epoch=epoch)\n",
        "        lr_scheduler.step()\n",
        "        test_auc = evaluate(model, val_dataloader, device=device)\n",
        "        if test_auc is not None:\n",
        "            print(\"AUC ON TEST: {:.4f}\".format(test_auc))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5zi3LMUwXao"
      },
      "source": [
        "## Step 5. Train model (30 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOIm5e6TT7Pm"
      },
      "source": [
        "Train the model for object detection on a training dataset and achieve a PR-AUC of at least 0.91 on a test dataset. You can use models from `torchvision`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AJ0Bi_JUHoe"
      },
      "source": [
        "It is mandatory to use augmentation for training to achieve the desired result on the test. Use the `torchvision.transforms` module or the [albumentations](https://albumentations.ai/) library. The latter library is especially convenient since it can calculate the new coordinates of bounding boxes itself after image transformations. We advise you to pay attention to this [tutorial](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Please note that the code written in the dataset above is only correct if you are using `albumentations`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfX1ayfrTgNO",
        "outputId": "af4f2e0c-f673-443a-d275-5ddb019cb370"
      },
      "source": [
        "import albumentations as A\n",
        "\n",
        "from albumentations.pytorch import ToTensor\n",
        "import cv2\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    # A.RandomCrop(width=229, height=229),\n",
        "\n",
        "    A.augmentations.transforms.CenterCrop (height=229, width=229),\n",
        "    # ToTensor(len(class2tag) + 1),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "val_transform = A.Compose([\n",
        "    # A.RandomCrop(width=229, height=229),\n",
        "\n",
        "    A.augmentations.transforms.CenterCrop (height=229, width=229),\n",
        "    # ToTensor(len(class2tag) + 1),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "# HINT: TRAIN TRANSFORM OBVIOUSLY SHOULD BE HARDER THAN THOSE FOR VALIDATION\n",
        "\n",
        "train_dataset = FruitDataset(\"./drive/MyDrive/Colab Notebooks/train_zip/train\", transform=None)\n",
        "val_dataset = FruitDataset(\"./drive/MyDrive/Colab Notebooks/test_zip/test\", transform=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB8YdPhS1NiH"
      },
      "source": [
        "# train_dataset[0][1]['labels'].to"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkEJrUVCntqj",
        "outputId": "9ae193ed-5a1d-400f-9858-1182231e5951"
      },
      "source": [
        "len(val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "rN_qlKqnwidK",
        "outputId": "8e7ef33f-6dfb-4505-dee0-8f3d142b47dd"
      },
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "num_classes = len(class2tag) + 1\n",
        "\n",
        "# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# backbone.out_channels = 229\n",
        "# model = FasterRCNN(backbone,\n",
        "#                    num_classes=2,\n",
        "#                    )\n",
        "\n",
        "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, rpn_nms_thresh=0.2)\n",
        "# try another model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "# HINT: YOU CAN USE torchvision.models AND torchvision.models.detection\n",
        "# READ OFFICIAL DOCS FOR MORE INFO\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE, num_workers=1, shuffle=True,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE, num_workers=1,\n",
        ")\n",
        "n_epochs = 2\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# Try Adam\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=3,\n",
        "                                                   gamma=0.1)\n",
        "\n",
        "train(model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device, n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-10f1712ff5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                    gamma=0.1)\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-233a1957014c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, lr_scheduler, device, n_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# model.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtest_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-233a1957014c>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\", line 83, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\", line 83, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\", line 63, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 1394, 2091] at entry 0 and [3, 430, 573] at entry 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c60A2kk0S8R9"
      },
      "source": [
        "Output the final quality of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0pjpdNGS3n-"
      },
      "source": [
        "evaluate(model, val_dataloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdAOThYUS-91"
      },
      "source": [
        "Draw predicted bounding boxes for any two images from the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCrtr-x9TA8G"
      },
      "source": [
        "it = iter(train_dataset)\n",
        "image, labels = next(it)\n",
        "image, labels = next(it)\n",
        "# image = image.reshape((image.shape[1], image.shape[2], image.shape[0]))\n",
        "pred = model(torch.Tensor(image).unsqueeze(0).to(device))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUQq3OxvTGti"
      },
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "# image = torchvision.transform.ToPILImage()(image)\n",
        "image2 = image.reshape((image.shape[1], image.shape[2], image.shape[0])).astype(np.uint8)\n",
        "image2 = T.ToPILImage()(image2)\n",
        "draw = ImageDraw.Draw(image2)\n",
        "for box in labels['boxes']:\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
        "\n",
        "for box in pred['boxes']:\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
        "image2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCGswsZG78c2"
      },
      "source": [
        "\n",
        "# from PIL import ImageDraw\n",
        "\n",
        "counter = 0\n",
        "for image, labels in train_dataset:\n",
        "  if counter >= 10:\n",
        "      break\n",
        "  pred = model(torch.Tensor(image).unsqueeze(0).to(device))[0]\n",
        "  # image = torchvision.transform.ToPILImage()(image)\n",
        "  image2 = image.reshape((image.shape[1], image.shape[2], image.shape[0])).astype(np.uint8)\n",
        "  image2 = T.ToPILImage()(image2)\n",
        "  draw = ImageDraw.Draw(image2)\n",
        "  for box in labels['boxes']:\n",
        "      draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
        "\n",
        "  for box in pred['boxes']:\n",
        "      draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
        "  display(image2)\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}