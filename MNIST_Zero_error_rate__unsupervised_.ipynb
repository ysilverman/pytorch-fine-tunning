{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ispITQWRDWi"
      },
      "source": [
        "# “Unsupervised” learning on MNIST data\n",
        "\n",
        "## Overview\n",
        "\n",
        "By running this notebook you can reproduce experiment results with training the model for classification of MNIST images using a very small number (10 images per each digit class) of labeled examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6CU7tgmSe4z"
      },
      "source": [
        "## Loading MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn3eLJcNli27"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision import datasets\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 20\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root='data', train=True,  download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=batch_size, num_workers=num_workers, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK3V_Ld2SjVa"
      },
      "source": [
        "##Label vectors\n",
        "\n",
        "Instead of labels provided in the original MNIST dataset, we use 10 normalized label vectors of 25 positive reals. Those label vectors should meet a simple criteria: each pair of vectors should have cosine similarity not bigger than 0.5. That maximizes a mutual distance between all the label vectors. In other words, the label vectors are 10 cluster centroids with the distance between each other bigger than a specific threshold (0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOVc4rdTdsr4"
      },
      "source": [
        "vector_labels = torch.tensor(\n",
        "  [\n",
        "    [0.0788, 0.1254, 0.0268, 0.2183, 0.3003, 0.3279, 0.0510, 0.0439, 0.3552,\n",
        "      0.0749, 0.0862, 0.3351, 0.0506, 0.1168, 0.1643, 0.1960, 0.2681, 0.2267,\n",
        "      0.1867, 0.3841, 0.0528, 0.1355, 0.0233, 0.0842, 0.2447], # digit 0\n",
        "    [0.0751, 0.1258, 0.2656, 0.0132, 0.0048, 0.0459, 0.0123, 0.0921, 0.0722,\n",
        "      0.4720, 0.0563, 0.0858, 0.1946, 0.0289, 0.0549, 0.0889, 0.0180, 0.0328,\n",
        "      0.2810, 0.0050, 0.2128, 0.0508, 0.6535, 0.0418, 0.2278],  # digit 1\n",
        "    [0.2016, 0.2286, 0.0246, 0.0350, 0.0284, 0.0113, 0.5795, 0.0695, 0.0989,\n",
        "      0.0515, 0.5213, 0.0253, 0.2721, 0.0178, 0.0095, 0.0061, 0.0243, 0.0309,\n",
        "      0.1076, 0.0285, 0.2499, 0.1712, 0.0729, 0.3033, 0.0366],  # digit 2\n",
        "    [0.0282, 0.1384, 0.0564, 0.0931, 0.0529, 0.0337, 0.1164, 0.1348, 0.0838,\n",
        "      0.0159, 0.3105, 0.0114, 0.1378, 0.0378, 0.0598, 0.7858, 0.0471, 0.1663,\n",
        "      0.0565, 0.0347, 0.0680, 0.2686, 0.0610, 0.2588, 0.0746],  # digit 3\n",
        "    [0.0368, 0.1348, 0.6532, 0.1854, 0.0323, 0.1109, 0.1187, 0.0222, 0.0124,\n",
        "      0.1473, 0.2992, 0.0402, 0.0864, 0.2666, 0.0860, 0.0477, 0.2516, 0.2643,\n",
        "      0.0793, 0.1082, 0.3297, 0.0828, 0.0344, 0.0016, 0.1499],  # digit 4\n",
        "    [0.4528, 0.1523, 0.0251, 0.0511, 0.1991, 0.0561, 0.1557, 0.6983, 0.0731,\n",
        "      0.1860, 0.0884, 0.0276, 0.1971, 0.1252, 0.1858, 0.0104, 0.0453, 0.1107,\n",
        "      0.0252, 0.0585, 0.0470, 0.1628, 0.1336, 0.0263, 0.1031],  # digit 5\n",
        "    [0.1442, 0.0414, 0.0260, 0.1051, 0.1011, 0.0484, 0.0940, 0.0715, 0.0459,\n",
        "      0.5043, 0.0312, 0.0336, 0.1456, 0.5837, 0.2772, 0.0289, 0.4086, 0.0950,\n",
        "      0.1037, 0.0274, 0.0142, 0.0024, 0.0353, 0.2247, 0.0419],  # digit 6\n",
        "    [0.0253, 0.0100, 0.0521, 0.1290, 0.1214, 0.1809, 0.0092, 0.0036, 0.0223,\n",
        "      0.0029, 0.0192, 0.0209, 0.5189, 0.1902, 0.2108, 0.0400, 0.0302, 0.1245,\n",
        "      0.4193, 0.0728, 0.0449, 0.5350, 0.2411, 0.1886, 0.0111],  # digit 7\n",
        "    [0.1489, 0.7321, 0.0011, 0.0065, 0.0215, 0.0462, 0.1617, 0.0958, 0.1073,\n",
        "      0.0546, 0.0956, 0.0459, 0.0222, 0.1168, 0.2183, 0.0008, 0.0317, 0.0330,\n",
        "      0.0749, 0.3990, 0.0605, 0.1267, 0.3394, 0.1195, 0.0142],  # digit 8\n",
        "    [0.5247, 0.0227, 0.2495, 0.0037, 0.1025, 0.2442, 0.0776, 0.0051, 0.0561,\n",
        "      0.0639, 0.0168, 0.1553, 0.1897, 0.0241, 0.0676, 0.1564, 0.0050, 0.0364,\n",
        "      0.0179, 0.0769, 0.0420, 0.0796, 0.2302, 0.5864, 0.2758]  # digit 9\n",
        "  ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFxJhbKcSrkM"
      },
      "source": [
        "## The model\n",
        "\n",
        "We use a simple model that contains 3 convolution layers with Relu activation function, each followed by a max-pooling layer.\n",
        "\n",
        "For a given image of 28 x 28 pixels with a handwritten digit, the model generates a vector of 25 positive reals.\n",
        "\n",
        "The loss function calculates the distance between the generated vector and the label vector associated with the class of a given image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HirDjIXML-bm"
      },
      "source": [
        "def loss_fn(x, y):\n",
        "  return (1 - nn.CosineSimilarity(eps=1e-6)(x, y)).sum() / x.size(0)\n",
        "\n",
        "class MNIST_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MNIST_Classifier, self).__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "      self.conv2 = nn.Conv2d(16, 8, 3, padding=2)\n",
        "      self.conv3 = nn.Conv2d(8,  1, 3, padding=2)\n",
        "\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "      return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv3.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3cyGLGfS7X1"
      },
      "source": [
        "## Initial training set\n",
        "\n",
        "We randomly choose 10 images per each class of digits from the MNIST training set and save in a separate training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlDmZFHbUFiX"
      },
      "source": [
        "def sort_train_loader(train_loader, sample_size = 10, seed = 0):\n",
        "  original_train_loader = False\n",
        "\n",
        "  if 'targets' in set(dir(train_loader.dataset)):\n",
        "    original_train_loader = True\n",
        "\n",
        "  if original_train_loader:\n",
        "    train_targets = train_loader.dataset.targets\n",
        "  else:\n",
        "    train_targets = train_loader.dataset.tensors[1]\n",
        "\n",
        "  train_loader_digits = []\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  for i in range(vector_labels.size(0)):\n",
        "    if original_train_loader:\n",
        "      ds = train_loader.dataset.data[train_targets == i].unsqueeze(1).float() / 255\n",
        "    else:\n",
        "      ds = train_loader.dataset.tensors[0][train_targets == i].unsqueeze(1).float() / 255\n",
        "\n",
        "    train_loader_digits.append(\n",
        "      torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(\n",
        "          ds[torch.randperm(ds.size(0))][0:sample_size],\n",
        "          train_targets[train_targets == i][0:sample_size]\n",
        "        ),\n",
        "        batch_size  = batch_size,\n",
        "        num_workers = num_workers,\n",
        "        shuffle     = True\n",
        "      )\n",
        "    )\n",
        "\n",
        "  return train_loader_digits\n",
        "\n",
        "train_loader_digits = sort_train_loader(train_loader, sample_size = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX-_PP5ATUAt"
      },
      "source": [
        "## Training initial model\n",
        "\n",
        "We use 10 separate models, one model per each class of images. We train each model on its own training set of only 10 images. Also as was mentioned, each model is associated with an appropriate label vector that represents one of 10 classes of images with handwritten digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iTyLDbpum10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "fc0f14d4-d825-46e2-e8ae-b8285a83d262"
      },
      "source": [
        "import copy\n",
        "\n",
        "def train_model(model, train_loader_digit, opt, n_epochs=20):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for data in train_loader_digit:\n",
        "      images, labels = data\n",
        "\n",
        "      opt.zero_grad()\n",
        "\n",
        "      out = model(images.to(device)).view(images.size(0), 25)\n",
        "\n",
        "      loss = loss_fn(out, torch.tensor([vector_labels[i].tolist() for i in labels]).to(device)) * images.size(0)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      opt.step()\n",
        "\n",
        "      train_loss += (loss.item() * images.size(0))\n",
        "\n",
        "    train_loss = train_loss / train_loader_digit.dataset.tensors[0].size(0)\n",
        "\n",
        "  return train_loss\n",
        "\n",
        "def choose_best_model(digit=0, epochs=50, min_loss=0.35, max_tries=100):\n",
        "  try_num = 0\n",
        "  best_model = None\n",
        "  best_loss = None\n",
        "\n",
        "  while try_num < max_tries:\n",
        "    model  = MNIST_Classifier().to(device)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "\n",
        "    model.init_weights()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss = train_model(model, train_loader_digits[digit], opt, epochs)\n",
        "\n",
        "    if loss < min_loss:\n",
        "      best_model = copy.deepcopy(model)\n",
        "\n",
        "      best_loss = loss\n",
        "\n",
        "      break\n",
        "\n",
        "    if best_loss is None or loss < best_loss:\n",
        "      best_model = copy.deepcopy(model)\n",
        "\n",
        "      best_loss = loss\n",
        "\n",
        "    try_num += 1\n",
        "\n",
        "  return best_model, best_loss\n",
        "\n",
        "def train_digit_models(epochs = 50, max_tries = 50, min_loss = 0.35, seed = 0):\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  digit_models = []\n",
        "\n",
        "  for digit in range(10):\n",
        "    print('Training the model for digit {} (training set size {})'.format(digit, train_loader_digits[digit].dataset.tensors[0].size(0)))\n",
        "\n",
        "    model, loss = choose_best_model(digit, epochs=epochs, min_loss=min_loss, max_tries=max_tries)\n",
        "\n",
        "    print('Best loss for {} digit: {:.6f}'.format(digit, loss))\n",
        "\n",
        "    digit_models.append(copy.deepcopy(model))\n",
        "\n",
        "  return digit_models\n",
        "\n",
        "vector_labels = vector_labels[torch.randperm(vector_labels.size(0))]\n",
        "\n",
        "digit_models = train_digit_models(epochs = 50, max_tries = 50, min_loss = 0.35)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model for digit 0 (training set size 10)\n",
            "Best loss for 0 digit: 0.164769\n",
            "Training the model for digit 1 (training set size 10)\n",
            "Best loss for 1 digit: 0.301143\n",
            "Training the model for digit 2 (training set size 10)\n",
            "Best loss for 2 digit: 0.152520\n",
            "Training the model for digit 3 (training set size 10)\n",
            "Best loss for 3 digit: 0.142487\n",
            "Training the model for digit 4 (training set size 10)\n",
            "Best loss for 4 digit: 0.170634\n",
            "Training the model for digit 5 (training set size 10)\n",
            "Best loss for 5 digit: 0.156419\n",
            "Training the model for digit 6 (training set size 10)\n",
            "Best loss for 6 digit: 0.647418\n",
            "Training the model for digit 7 (training set size 10)\n",
            "Best loss for 7 digit: 0.243345\n",
            "Training the model for digit 8 (training set size 10)\n",
            "Best loss for 8 digit: 0.284011\n",
            "Training the model for digit 9 (training set size 10)\n",
            "Best loss for 9 digit: 0.087508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quy5DQYATZDM"
      },
      "source": [
        "## Classification of digit images\n",
        "\n",
        "Our initial model has a very limited knowledge about handwritten images. In the next step we apply our initial model to the whole training set (60K images) and label the images for which the model returns cosine similarity bigger than 0.895 threshold value.\n",
        "\n",
        "It will give us new labeled image examples that can be used for training the model that will have a much better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k84u4urSfvf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "904d46a5-fa5a-4828-b014-5a7d06b8ed06"
      },
      "source": [
        "def classify_images(images, labels):\n",
        "  loss = []\n",
        "\n",
        "  for j in range(len(digit_models)):\n",
        "    out = digit_models[j](images.to(device)).view(images.size(0), 25)\n",
        "\n",
        "    loss.append(nn.CosineSimilarity(eps=1e-6)(out, torch.tensor([vector_labels[i].tolist() for i in labels]).to(device)).tolist())\n",
        "\n",
        "  return loss\n",
        "\n",
        "classified_cnt = 0\n",
        "\n",
        "min_similarity = 0.895\n",
        "\n",
        "i = 0\n",
        "\n",
        "selected_images = []\n",
        "selected_labels = []\n",
        "\n",
        "for data in train_loader:\n",
        "  images, labels = data\n",
        "\n",
        "  loss = classify_images(images, labels)\n",
        "\n",
        "  loss_tr = torch.tensor(loss).t()\n",
        "\n",
        "  selected_ind = torch.arange(images.size(0))[(loss_tr > min_similarity).any(1)]\n",
        "\n",
        "  selected_images.append(images[selected_ind])\n",
        "  selected_labels.append(loss_tr[selected_ind,:].argsort(1)[:,-1])\n",
        "\n",
        "  if (loss_tr[selected_ind,:].argsort(1)[:,-1] - labels[selected_ind]).sum() > 0:\n",
        "    print(loss_tr[selected_ind,:].argsort(1)[:,-1])\n",
        "    print(labels[selected_ind])\n",
        "  else:\n",
        "    classified_cnt += selected_ind.size(0)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print('Classified {} images'.format(classified_cnt))\n",
        "\n",
        "  i += 1\n",
        "\n",
        "print('Total classified images {}'.format(classified_cnt))\n",
        "\n",
        "selected_images = torch.cat(selected_images) * 255\n",
        "selected_labels = torch.cat(selected_labels)\n",
        "\n",
        "new_train_loader = torch.utils.data.DataLoader(\n",
        "  torch.utils.data.TensorDataset(\n",
        "    selected_images.squeeze(1),\n",
        "    selected_labels\n",
        "  ),\n",
        "  batch_size  = batch_size,\n",
        "  num_workers = num_workers,\n",
        "  shuffle     = True\n",
        ")\n",
        "\n",
        "selected_labels.unique(return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classified 15 images\n",
            "Classified 1396 images\n",
            "Classified 2757 images\n",
            "Classified 4105 images\n",
            "Classified 5486 images\n",
            "Classified 6852 images\n",
            "Classified 8211 images\n",
            "Classified 9544 images\n",
            "Classified 10880 images\n",
            "Classified 12258 images\n",
            "Classified 13633 images\n",
            "Classified 15018 images\n",
            "Classified 16378 images\n",
            "Classified 17733 images\n",
            "Classified 19095 images\n",
            "Classified 20452 images\n",
            "Classified 21797 images\n",
            "Classified 23182 images\n",
            "Classified 24528 images\n",
            "Classified 25891 images\n",
            "Classified 27223 images\n",
            "Classified 28606 images\n",
            "Classified 29961 images\n",
            "Classified 31329 images\n",
            "Classified 32685 images\n",
            "Classified 34061 images\n",
            "Classified 35460 images\n",
            "Classified 36794 images\n",
            "Classified 38162 images\n",
            "Classified 39532 images\n",
            "Total classified images 40871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
              " tensor([5300, 6727, 2535, 4623, 2910, 4949,  770, 6259, 3092, 3706]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq7AvMegUbfK"
      },
      "source": [
        "## Tranining the model for classification\n",
        "\n",
        "Now we have a much bigger data set with the labels, so we can use more digit image examples and train the more accurate model.\n",
        "\n",
        "Here we create a new training set with 500 randomly selected images per each digit class and train the model from scratch on a new training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yCA7ND9xkEN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "d8426616-f57b-44b3-8a7a-e51143e0d377"
      },
      "source": [
        "train_loader_digits = sort_train_loader(new_train_loader, sample_size = 500, seed = 1)\n",
        "\n",
        "vector_labels = vector_labels[torch.randperm(vector_labels.size(0))]\n",
        "\n",
        "digit_models = train_digit_models(epochs = 20, max_tries = 10, min_loss = 0.55, seed = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model for digit 0 (training set size 500)\n",
            "Best loss for 0 digit: 0.153815\n",
            "Training the model for digit 1 (training set size 500)\n",
            "Best loss for 1 digit: 0.466718\n",
            "Training the model for digit 2 (training set size 500)\n",
            "Best loss for 2 digit: 0.573049\n",
            "Training the model for digit 3 (training set size 500)\n",
            "Best loss for 3 digit: 0.322496\n",
            "Training the model for digit 4 (training set size 500)\n",
            "Best loss for 4 digit: 0.485401\n",
            "Training the model for digit 5 (training set size 500)\n",
            "Best loss for 5 digit: 0.814577\n",
            "Training the model for digit 6 (training set size 500)\n",
            "Best loss for 6 digit: 0.486279\n",
            "Training the model for digit 7 (training set size 500)\n",
            "Best loss for 7 digit: 0.188087\n",
            "Training the model for digit 8 (training set size 500)\n",
            "Best loss for 8 digit: 0.079691\n",
            "Training the model for digit 9 (training set size 500)\n",
            "Best loss for 9 digit: 0.444722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc7ySmOuUhLv"
      },
      "source": [
        "## Testing the model\n",
        "\n",
        "In the eval_model() function we apply 10 models to a given image and calculate cosine similarity between generated vector and the label vector associated with each model. It allows to classify the image by choosing the label with highest cosine similarity value. We run the eval_model() function against the test dataset (10K images) and also against the whole training set (60K images). Note that we trained our models on the limited set of images (500 images per each class) randomly selected from the training set. Our actual training set is only a fraction (10 * 500 / 60000) of the whole training set. It means that more than 90% of the images from training set have not seen by our models during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m_ybtZVoaiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bc584730-afb6-4bb8-d667-7ae9c74868aa"
      },
      "source": [
        "def eval_model(dataset_loader):\n",
        "  miss_classified = 0\n",
        "\n",
        "  for data in dataset_loader:\n",
        "    images, labels = data\n",
        "\n",
        "    loss = []\n",
        "\n",
        "    for j in range(len(digit_models)):\n",
        "      out = digit_models[j](images.to(device)).view(images.size(0), 25)\n",
        "\n",
        "      loss.append(nn.CosineSimilarity(eps=1e-6)(out, torch.tensor([vector_labels[i].tolist() for i in labels]).to(device)).tolist())\n",
        "\n",
        "    miss_classified += (torch.tensor(loss).t().argsort()[:,-1] != labels).int().sum()\n",
        "\n",
        "  return miss_classified\n",
        "\n",
        "print('Test set: miss-classified {} in {} images'.format(eval_model(test_loader), test_loader.dataset.data.size(0)))\n",
        "\n",
        "print('Train set: miss-classified {} in {} images'.format(eval_model(train_loader), train_loader.dataset.data.size(0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: miss-classified 1 in 10000 images\n",
            "Train set: miss-classified 12 in 60000 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRmbi2soXahp"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Starting with a few labeled image examples we trained the model that allowed us to label a big number (more than 40K) of images from a training set (60K images). Then using additional labeled images we trained a new, more accurate model that demonstrated near 100% accuracy on the test set.\n",
        "\n",
        "This approach shows how having a very few labeled examples, we can build a model that generalizes very well to unseen examples and provides an accuracy of a human level.\n"
      ]
    }
  ]
}